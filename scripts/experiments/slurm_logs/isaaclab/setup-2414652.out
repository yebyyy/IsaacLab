/srv/flash1/xye87/.apptainer/tmp
Warning: running in conda env, please deactivate before executing this script
If conda is desired please source setup_conda_env.sh in your python 3.11 conda env and run python normally
[Info] [carb] Logging to file: /isaac-sim/kit/logs/Kit/Isaac-Sim/5.1/kit_20260208_001040.log
2026-02-08T05:10:51Z [11,097ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.
2026-02-08T05:10:51Z [11,101ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.5]) (impl: carb.windowing-glfw.plugin)
2026-02-08T05:10:51Z [11,289ms] [Warning] [omni.platforminfo.plugin] failed to open the default display.  Can't verify X Server version.
2026-02-08T05:10:51Z [11,370ms] [Warning] [carb.cudainterop.plugin] CUDA_VISIBLE_DEVICES environment variable is set.
2026-02-08T05:10:51Z [11,377ms] [Warning] [carb.cudainterop.plugin] Note CUDA device enumeration and Omniverse device enumeration are different.
2026-02-08T05:10:51Z [11,385ms] [Warning] [carb.cudainterop.plugin] Setting CUDA_VISIBLE_DEVICES can lead to undesired behavior or crashes.

|---------------------------------------------------------------------------------------------|
| Driver Version: 560.35.03     | Graphics API: Vulkan
|=============================================================================================|
| GPU | Name                             | Active | LDA | GPU Memory | Vendor-ID | LUID       |
|     |                                  |        |     |            | Device-ID | UUID       |
|     |                                  |        |     |            | Bus-ID    |            |
|---------------------------------------------------------------------------------------------|
| 0   | NVIDIA A40                       | Yes: 0 |     | 46068   MB | 10de      | 0          |
|     |                                  |        |     |            | 2235      | 4d4432ce.. |
|     |                                  |        |     |            | 1         |            |
|=============================================================================================|
| OS: 24.04.2 LTS (Noble Numbat) ubuntu, Version: 24.04.2, Kernel: 5.4.0-216-generic
| Processor: AMD EPYC 7452 32-Core Processor
| Cores: 64 | Logical Cores: 128
|---------------------------------------------------------------------------------------------|
| Total Memory (MB): 515820 | Free Memory: 35801
| Total Page/Swap (MB): 2047 | Free Page/Swap: 1408
|---------------------------------------------------------------------------------------------|
2026-02-08T05:10:53Z [13,242ms] [Warning] [gpu.foundation.plugin] ECC is enabled on physical device 0
2026-02-08T05:10:59Z [19,021ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.
2026-02-08T05:10:59Z [19,021ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.5]) (impl: carb.windowing-glfw.plugin)
2026-02-08T05:10:59Z [19,075ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.
2026-02-08T05:10:59Z [19,075ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.5]) (impl: carb.windowing-glfw.plugin)
2026-02-08T05:10:59Z [19,083ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.
2026-02-08T05:10:59Z [19,083ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.5]) (impl: carb.windowing-glfw.plugin)
2026-02-08T05:10:59Z [19,146ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.
2026-02-08T05:10:59Z [19,146ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.5]) (impl: carb.windowing-glfw.plugin)
2026-02-08T05:10:59Z [19,149ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.
2026-02-08T05:10:59Z [19,149ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.5]) (impl: carb.windowing-glfw.plugin)
2026-02-08T05:10:59Z [19,155ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.
2026-02-08T05:10:59Z [19,155ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.5]) (impl: carb.windowing-glfw.plugin)
2026-02-08T05:11:01Z [21,021ms] [Warning] [omni.log] Source: omni.hydra was already registered.
2026-02-08T05:11:04Z [23,648ms] [Warning] [carb] Acquiring non optional plugin interface which is not listed as dependency: [omni::physx::IPhysxBenchmarks v1.0] (plugin: <default plugin>), by client: omni.physics.physx.plugin. Add it to CARB_PLUGIN_IMPL_DEPS() macro of a client.
2026-02-08T05:11:04Z [23,685ms] [Warning] [omni.isaac.dynamic_control] omni.isaac.dynamic_control is deprecated as of Isaac Sim 4.5. No action is needed from end-users.
2026-02-08T05:11:14Z [34,309ms] [Warning] [pxr.Semantics] pxr.Semantics is deprecated - please use Semantics instead
2026-02-08T05:11:17Z [36,663ms] [Warning] [omni.graph.core.plugin] Found duplicate of category 'Replicator' - was 'Annotators', adding 'Fabric Reader'
2026-02-08T05:11:17Z [36,663ms] [Warning] [omni.graph.core.plugin] Category 'Replicator' not accepted on node type 'omni.replicator.core.FabricReader' in extension 'omni.replicator.core'
2026-02-08T05:11:17Z [36,669ms] [Warning] [omni.replicator.core.scripts.extension] No material configuration file, adding configuration to material settings directly.
2026-02-08T05:11:24Z [44,163ms] [Warning] [carb.graphics-vulkan.plugin] Trying to initialize NGX without vkGetInstanceProcAddr and vkGetDeviceProcAddr pointer-to-functions.
[INFO][AppLauncher]: Using device: cuda:0
[INFO][AppLauncher]: Loading experience file: /workspace/isaaclab/apps/isaaclab.python.headless.rendering.kit
[INFO]: Parsing configuration from: isaaclab_tasks.manager_based.classic.cartpole.cartpole_env_cfg:CartpoleEnvCfg
[INFO]: Parsing configuration from: isaaclab_tasks.manager_based.classic.cartpole.agents.rsl_rl_ppo_cfg:CartpolePPORunnerCfg
[INFO] Logging experiment in directory: /workspace/isaaclab/logs/rsl_rl/cartpole
Exact experiment name requested from command line: 2026-02-08_00-11-36

[36m======================================================================================[0m
[36m[1m[INFO][IsaacLab]: Logging to file: /tmp/isaaclab/logs/isaaclab_2026-02-08_00-11-36.log[0m
[36m======================================================================================[0m

[33m00:11:36 [simulation_context.py] WARNING: The `enable_external_forces_every_iteration` parameter in the PhysxCfg is set to False. If you are experiencing noisy velocities, consider enabling this flag. You may need to slightly increase the number of velocity iterations (setting it to 1 or 2 rather than 0), together with this flag, to improve the accuracy of velocity updates.[0m
[INFO]: Base environment:
	Environment device    : cuda:0
	Environment seed      : 42
	Physics step-size     : 0.008333333333333333
	Rendering step-size   : 0.016666666666666666
	Environment step-size : 0.016666666666666666
[INFO]: Time taken for scene creation : 1.166993 seconds
[INFO]: Scene manager:  <class InteractiveScene>
	Number of environments: 2
	Environment spacing   : 4.0
	Source prim name      : /World/envs/env_0
	Global prim paths     : []
	Replicate physics     : True
[INFO]: Starting the simulation. This may take a few seconds. Please wait...
[INFO]: Time taken for simulation start : 7.274235 seconds
[INFO] Command Manager:  <CommandManager> contains 0 active terms.
+------------------------+
|  Active Command Terms  |
+--------+-------+-------+
| Index  | Name  |  Type |
+--------+-------+-------+
+--------+-------+-------+

[INFO] Event Manager:  <EventManager> contains 1 active terms.
+-------------------------------------+
| Active Event Terms in Mode: 'reset' |
+---------+---------------------------+
|  Index  | Name                      |
+---------+---------------------------+
|    0    | reset_cart_position       |
|    1    | reset_pole_position       |
+---------+---------------------------+

[INFO] Recorder Manager:  <RecorderManager> contains 0 active terms.
+---------------------+
| Active Recorder Terms |
+-----------+---------+
|   Index   | Name    |
+-----------+---------+
+-----------+---------+

[INFO] Action Manager:  <ActionManager> contains 1 active terms.
+----------------------------------+
|  Active Action Terms (shape: 1)  |
+-------+--------------+-----------+
| Index | Name         | Dimension |
+-------+--------------+-----------+
|   0   | joint_effort |         1 |
+-------+--------------+-----------+

[INFO] Observation Manager: <ObservationManager> contains 1 groups.
+------------------------------------------------------+
| Active Observation Terms in Group: 'policy' (shape: (4,)) |
+------------+----------------------------+------------+
|   Index    | Name                       |   Shape    |
+------------+----------------------------+------------+
|     0      | joint_pos_rel              |    (2,)    |
|     1      | joint_vel_rel              |    (2,)    |
+------------+----------------------------+------------+

[INFO] Termination Manager:  <TerminationManager> contains 2 active terms.
+---------------------------------------+
|        Active Termination Terms       |
+-------+--------------------+----------+
| Index | Name               | Time Out |
+-------+--------------------+----------+
|   0   | time_out           |   True   |
|   1   | cart_out_of_bounds |  False   |
+-------+--------------------+----------+

[INFO] Reward Manager:  <RewardManager> contains 5 active terms.
+------------------------------+
|     Active Reward Terms      |
+-------+-------------+--------+
| Index | Name        | Weight |
+-------+-------------+--------+
|   0   | alive       |    1.0 |
|   1   | terminating |   -2.0 |
|   2   | pole_pos    |   -1.0 |
|   3   | cart_vel    |  -0.01 |
|   4   | pole_vel    | -0.005 |
+-------+-------------+--------+

[INFO] Curriculum Manager:  <CurriculumManager> contains 0 active terms.
+----------------------+
| Active Curriculum Terms |
+-----------+----------+
|   Index   | Name     |
+-----------+----------+
+-----------+----------+

[INFO]: Completed setting up the environment...
[INFO] Recording videos during training.
        video_folder: /workspace/isaaclab/logs/rsl_rl/cartpole/2026-02-08_00-11-36/videos/train
        step_trigger: lambda step: step % args_cli.video_interval == 0
        video_length: 1000
        disable_logger: True
--------------------------------------------------------------------------------
Resolved observation sets: 
	 policy :  ['policy']
	 critic :  ['policy']
--------------------------------------------------------------------------------
Actor MLP: MLP(
  (0): Linear(in_features=4, out_features=32, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=32, out_features=32, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=32, out_features=1, bias=True)
)
Critic MLP: MLP(
  (0): Linear(in_features=4, out_features=32, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=32, out_features=32, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=32, out_features=1, bias=True)
)
################################################################################
                       [1m Learning iteration 0/100 [0m                       

                       Computation: 5 steps/s (collection: 4.721s, learning 0.876s)
             Mean action noise std: 1.02
                    value_function: 0.0112
                         surrogate: -0.0388
                           entropy: 1.4324
              Episode_Reward/alive: 0.0000
        Episode_Reward/terminating: 0.0000
           Episode_Reward/pole_pos: 0.0000
           Episode_Reward/cart_vel: 0.0000
           Episode_Reward/pole_vel: 0.0000
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32
                    Iteration time: 5.60s
                      Time elapsed: 00:00:05
                               ETA: 00:09:19

Could not find git repository in /isaac-sim/kit/python/lib/python3.11/site-packages/rsl_rl/__init__.py. Skipping.
Storing git diff for 'isaaclab' in: /workspace/isaaclab/logs/rsl_rl/cartpole/2026-02-08_00-11-36/git/isaaclab.diff
################################################################################
                       [1m Learning iteration 1/100 [0m                       

                       Computation: 32 steps/s (collection: 0.838s, learning 0.152s)
             Mean action noise std: 1.02
                    value_function: 0.1205
                         surrogate: -0.0034
                           entropy: 1.4368
              Episode_Reward/alive: 0.0000
        Episode_Reward/terminating: 0.0000
           Episode_Reward/pole_pos: 0.0000
           Episode_Reward/cart_vel: 0.0000
           Episode_Reward/pole_vel: 0.0000
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64
                    Iteration time: 0.99s
                      Time elapsed: 00:00:06
                               ETA: 00:05:26

################################################################################
                       [1m Learning iteration 2/100 [0m                       

                       Computation: 32 steps/s (collection: 0.834s, learning 0.144s)
             Mean action noise std: 1.02
                    value_function: 0.3360
                         surrogate: -0.0672
                           entropy: 1.4386
              Episode_Reward/alive: 0.0000
        Episode_Reward/terminating: 0.0000
           Episode_Reward/pole_pos: 0.0000
           Episode_Reward/cart_vel: 0.0000
           Episode_Reward/pole_vel: 0.0000
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96
                    Iteration time: 0.98s
                      Time elapsed: 00:00:07
                               ETA: 00:04:07

################################################################################
                       [1m Learning iteration 3/100 [0m                       

                       Computation: 32 steps/s (collection: 0.832s, learning 0.150s)
             Mean action noise std: 1.02
                    value_function: 0.0438
                         surrogate: -0.0181
                           entropy: 1.4372
              Episode_Reward/alive: 0.0000
        Episode_Reward/terminating: 0.0000
           Episode_Reward/pole_pos: 0.0000
           Episode_Reward/cart_vel: 0.0000
           Episode_Reward/pole_vel: 0.0000
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128
                    Iteration time: 0.98s
                      Time elapsed: 00:00:08
                               ETA: 00:03:27

################################################################################
                       [1m Learning iteration 4/100 [0m                       

                       Computation: 32 steps/s (collection: 0.841s, learning 0.140s)
             Mean action noise std: 1.02
                    value_function: 0.3423
                         surrogate: -0.0020
                           entropy: 1.4358
              Episode_Reward/alive: 0.0000
        Episode_Reward/terminating: 0.0000
           Episode_Reward/pole_pos: 0.0000
           Episode_Reward/cart_vel: 0.0000
           Episode_Reward/pole_vel: 0.0000
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160
                    Iteration time: 0.98s
                      Time elapsed: 00:00:09
                               ETA: 00:03:02

################################################################################
                       [1m Learning iteration 5/100 [0m                       

                       Computation: 32 steps/s (collection: 0.836s, learning 0.145s)
             Mean action noise std: 1.01
                    value_function: 0.0344
                         surrogate: -0.0256
                           entropy: 1.4342
              Episode_Reward/alive: 0.0000
        Episode_Reward/terminating: 0.0000
           Episode_Reward/pole_pos: 0.0000
           Episode_Reward/cart_vel: 0.0000
           Episode_Reward/pole_vel: 0.0000
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 192
                    Iteration time: 0.98s
                      Time elapsed: 00:00:10
                               ETA: 00:02:46

################################################################################
                       [1m Learning iteration 6/100 [0m                       

                       Computation: 32 steps/s (collection: 0.838s, learning 0.147s)
             Mean action noise std: 1.01
                    value_function: 0.1173
                         surrogate: -0.0190
                           entropy: 1.4331
              Episode_Reward/alive: 0.0000
        Episode_Reward/terminating: 0.0000
           Episode_Reward/pole_pos: 0.0000
           Episode_Reward/cart_vel: 0.0000
           Episode_Reward/pole_vel: 0.0000
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 224
                    Iteration time: 0.98s
                      Time elapsed: 00:00:11
                               ETA: 00:02:34

################################################################################
                       [1m Learning iteration 7/100 [0m                       

                       Computation: 32 steps/s (collection: 0.842s, learning 0.142s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.2208
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 1.4310
                       Mean reward: -3.84
               Mean episode length: 121.00
              Episode_Reward/alive: 0.2000
        Episode_Reward/terminating: -0.0033
           Episode_Reward/pole_pos: -0.5733
           Episode_Reward/cart_vel: -0.0046
           Episode_Reward/pole_vel: -0.0031
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 256
                    Iteration time: 0.98s
                      Time elapsed: 00:00:12
                               ETA: 00:02:25

################################################################################
                       [1m Learning iteration 8/100 [0m                       

                       Computation: 32 steps/s (collection: 0.838s, learning 0.146s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.1525
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 1.4247
                       Mean reward: -3.84
               Mean episode length: 121.00
              Episode_Reward/alive: 0.4000
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -1.1467
           Episode_Reward/cart_vel: -0.0091
           Episode_Reward/pole_vel: -0.0062
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 288
                    Iteration time: 0.98s
                      Time elapsed: 00:00:13
                               ETA: 00:02:17

################################################################################
                       [1m Learning iteration 9/100 [0m                       

                       Computation: 32 steps/s (collection: 0.837s, learning 0.139s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.1243
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 1.3989
                       Mean reward: -3.84
               Mean episode length: 121.00
              Episode_Reward/alive: 0.4000
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -1.1467
           Episode_Reward/cart_vel: -0.0091
           Episode_Reward/pole_vel: -0.0062
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 320
                    Iteration time: 0.98s
                      Time elapsed: 00:00:14
                               ETA: 00:02:11

################################################################################
                      [1m Learning iteration 10/100 [0m                       

                       Computation: 32 steps/s (collection: 0.838s, learning 0.143s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.2688
               Mean surrogate loss: -0.0202
                 Mean entropy loss: 1.3967
                       Mean reward: -3.84
               Mean episode length: 121.00
              Episode_Reward/alive: 0.4000
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -1.1467
           Episode_Reward/cart_vel: -0.0091
           Episode_Reward/pole_vel: -0.0062
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 352
                    Iteration time: 0.98s
                      Time elapsed: 00:00:15
                               ETA: 00:02:06

################################################################################
                      [1m Learning iteration 11/100 [0m                       

                       Computation: 32 steps/s (collection: 0.840s, learning 0.146s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.3208
               Mean surrogate loss: -0.0186
                 Mean entropy loss: 1.3778
                       Mean reward: -3.84
               Mean episode length: 121.00
              Episode_Reward/alive: 0.4000
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -1.1467
           Episode_Reward/cart_vel: -0.0091
           Episode_Reward/pole_vel: -0.0062
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 384
                    Iteration time: 0.99s
                      Time elapsed: 00:00:16
                               ETA: 00:02:01

################################################################################
                      [1m Learning iteration 12/100 [0m                       

                       Computation: 32 steps/s (collection: 0.838s, learning 0.144s)
             Mean action noise std: 0.95
          Mean value_function loss: 0.5770
               Mean surrogate loss: 0.0159
                 Mean entropy loss: 1.3593
                       Mean reward: -3.84
               Mean episode length: 121.00
              Episode_Reward/alive: 0.4000
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -1.1467
           Episode_Reward/cart_vel: -0.0091
           Episode_Reward/pole_vel: -0.0062
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 416
                    Iteration time: 0.98s
                      Time elapsed: 00:00:17
                               ETA: 00:01:57

################################################################################
                      [1m Learning iteration 13/100 [0m                       

                       Computation: 32 steps/s (collection: 0.838s, learning 0.145s)
             Mean action noise std: 0.95
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0359
                 Mean entropy loss: 1.3658
                       Mean reward: -3.84
               Mean episode length: 121.00
              Episode_Reward/alive: 0.4000
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -1.1467
           Episode_Reward/cart_vel: -0.0091
           Episode_Reward/pole_vel: -0.0062
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 448
                    Iteration time: 0.98s
                      Time elapsed: 00:00:18
                               ETA: 00:01:54

################################################################################
                      [1m Learning iteration 14/100 [0m                       

                       Computation: 32 steps/s (collection: 0.850s, learning 0.143s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0173
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 1.3682
                       Mean reward: -6.38
               Mean episode length: 180.50
              Episode_Reward/alive: 0.4250
        Episode_Reward/terminating: -0.0063
           Episode_Reward/pole_pos: -1.2347
           Episode_Reward/cart_vel: -0.0094
           Episode_Reward/pole_vel: -0.0067
      Episode_Termination/time_out: 0.0312
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 480
                    Iteration time: 0.99s
                      Time elapsed: 00:00:19
                               ETA: 00:01:51

################################################################################
                      [1m Learning iteration 15/100 [0m                       

                       Computation: 32 steps/s (collection: 0.851s, learning 0.145s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.1266
               Mean surrogate loss: -0.0358
                 Mean entropy loss: 1.3770
                       Mean reward: -6.38
               Mean episode length: 180.50
              Episode_Reward/alive: 0.8000
        Episode_Reward/terminating: 0.0000
           Episode_Reward/pole_pos: -2.5551
           Episode_Reward/cart_vel: -0.0135
           Episode_Reward/pole_vel: -0.0139
      Episode_Termination/time_out: 0.5000
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 512
                    Iteration time: 1.00s
                      Time elapsed: 00:00:20
                               ETA: 00:01:48

################################################################################
                      [1m Learning iteration 16/100 [0m                       

                       Computation: 31 steps/s (collection: 0.856s, learning 0.147s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0997
               Mean surrogate loss: -0.0222
                 Mean entropy loss: 1.3758
                       Mean reward: -6.38
               Mean episode length: 180.50
              Episode_Reward/alive: 0.8000
        Episode_Reward/terminating: 0.0000
           Episode_Reward/pole_pos: -2.5551
           Episode_Reward/cart_vel: -0.0135
           Episode_Reward/pole_vel: -0.0139
      Episode_Termination/time_out: 0.5000
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 544
                    Iteration time: 1.00s
                      Time elapsed: 00:00:21
                               ETA: 00:01:45

################################################################################
                      [1m Learning iteration 17/100 [0m                       

                       Computation: 32 steps/s (collection: 0.832s, learning 0.145s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.5987
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 1.3783
                       Mean reward: -6.38
               Mean episode length: 180.50
              Episode_Reward/alive: 0.8000
        Episode_Reward/terminating: 0.0000
           Episode_Reward/pole_pos: -2.5551
           Episode_Reward/cart_vel: -0.0135
           Episode_Reward/pole_vel: -0.0139
      Episode_Termination/time_out: 0.5000
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 576
                    Iteration time: 0.98s
                      Time elapsed: 00:00:22
                               ETA: 00:01:43

################################################################################
                      [1m Learning iteration 18/100 [0m                       

                       Computation: 32 steps/s (collection: 0.839s, learning 0.149s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.0335
               Mean surrogate loss: -0.0205
                 Mean entropy loss: 1.3893
                       Mean reward: -6.38
               Mean episode length: 180.50
              Episode_Reward/alive: 0.8000
        Episode_Reward/terminating: 0.0000
           Episode_Reward/pole_pos: -2.5551
           Episode_Reward/cart_vel: -0.0135
           Episode_Reward/pole_vel: -0.0139
      Episode_Termination/time_out: 0.5000
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 608
                    Iteration time: 0.99s
                      Time elapsed: 00:00:23
                               ETA: 00:01:40

################################################################################
                      [1m Learning iteration 19/100 [0m                       

                       Computation: 32 steps/s (collection: 0.828s, learning 0.145s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.1773
               Mean surrogate loss: -0.0197
                 Mean entropy loss: 1.3915
                       Mean reward: -6.38
               Mean episode length: 180.50
              Episode_Reward/alive: 0.8000
        Episode_Reward/terminating: 0.0000
           Episode_Reward/pole_pos: -2.5551
           Episode_Reward/cart_vel: -0.0135
           Episode_Reward/pole_vel: -0.0139
      Episode_Termination/time_out: 0.5000
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 640
                    Iteration time: 0.97s
                      Time elapsed: 00:00:24
                               ETA: 00:01:38

################################################################################
                      [1m Learning iteration 20/100 [0m                       

                       Computation: 32 steps/s (collection: 0.827s, learning 0.151s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.4178
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 1.3926
                       Mean reward: -6.38
               Mean episode length: 180.50
              Episode_Reward/alive: 0.8000
        Episode_Reward/terminating: 0.0000
           Episode_Reward/pole_pos: -2.5551
           Episode_Reward/cart_vel: -0.0135
           Episode_Reward/pole_vel: -0.0139
      Episode_Termination/time_out: 0.5000
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 672
                    Iteration time: 0.98s
                      Time elapsed: 00:00:25
                               ETA: 00:01:36

################################################################################
                      [1m Learning iteration 21/100 [0m                       

                       Computation: 32 steps/s (collection: 0.838s, learning 0.148s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.1327
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 1.3833
                       Mean reward: -6.38
               Mean episode length: 180.50
              Episode_Reward/alive: 0.8000
        Episode_Reward/terminating: 0.0000
           Episode_Reward/pole_pos: -2.5551
           Episode_Reward/cart_vel: -0.0135
           Episode_Reward/pole_vel: -0.0139
      Episode_Termination/time_out: 0.5000
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 704
                    Iteration time: 0.99s
                      Time elapsed: 00:00:26
                               ETA: 00:01:34

################################################################################
                      [1m Learning iteration 22/100 [0m                       

                       Computation: 32 steps/s (collection: 0.832s, learning 0.147s)
             Mean action noise std: 0.95
          Mean value_function loss: 0.1532
               Mean surrogate loss: -0.0326
                 Mean entropy loss: 1.3680
                       Mean reward: -6.38
               Mean episode length: 180.50
              Episode_Reward/alive: 0.8000
        Episode_Reward/terminating: 0.0000
           Episode_Reward/pole_pos: -2.5551
           Episode_Reward/cart_vel: -0.0135
           Episode_Reward/pole_vel: -0.0139
      Episode_Termination/time_out: 0.5000
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 736
                    Iteration time: 0.98s
                      Time elapsed: 00:00:27
                               ETA: 00:01:32

################################################################################
                      [1m Learning iteration 23/100 [0m                       

                       Computation: 32 steps/s (collection: 0.839s, learning 0.140s)
             Mean action noise std: 0.95
          Mean value_function loss: 2.3293
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 1.3645
                       Mean reward: -8.49
               Mean episode length: 206.67
              Episode_Reward/alive: 0.8187
        Episode_Reward/terminating: -0.0021
           Episode_Reward/pole_pos: -2.8084
           Episode_Reward/cart_vel: -0.0143
           Episode_Reward/pole_vel: -0.0138
      Episode_Termination/time_out: 0.5000
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 768
                    Iteration time: 0.98s
                      Time elapsed: 00:00:28
                               ETA: 00:01:30

################################################################################
                      [1m Learning iteration 24/100 [0m                       

                       Computation: 32 steps/s (collection: 0.830s, learning 0.145s)
             Mean action noise std: 0.94
          Mean value_function loss: 1.1703
               Mean surrogate loss: -0.0159
                 Mean entropy loss: 1.3666
                       Mean reward: -8.49
               Mean episode length: 206.67
              Episode_Reward/alive: 0.8600
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -3.3657
           Episode_Reward/cart_vel: -0.0160
           Episode_Reward/pole_vel: -0.0136
      Episode_Termination/time_out: 0.5000
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 800
                    Iteration time: 0.97s
                      Time elapsed: 00:00:29
                               ETA: 00:01:28

################################################################################
                      [1m Learning iteration 25/100 [0m                       

                       Computation: 32 steps/s (collection: 0.849s, learning 0.147s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.5159
               Mean surrogate loss: -0.0196
                 Mean entropy loss: 1.3599
                       Mean reward: -8.49
               Mean episode length: 206.67
              Episode_Reward/alive: 0.8600
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -3.3657
           Episode_Reward/cart_vel: -0.0160
           Episode_Reward/pole_vel: -0.0136
      Episode_Termination/time_out: 0.5000
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 832
                    Iteration time: 1.00s
                      Time elapsed: 00:00:30
                               ETA: 00:01:27

################################################################################
                      [1m Learning iteration 26/100 [0m                       

                       Computation: 32 steps/s (collection: 0.850s, learning 0.145s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0969
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 1.3564
                       Mean reward: -8.49
               Mean episode length: 206.67
              Episode_Reward/alive: 0.8600
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -3.3657
           Episode_Reward/cart_vel: -0.0160
           Episode_Reward/pole_vel: -0.0136
      Episode_Termination/time_out: 0.5000
Episode_Termination/cart_out_of_bounds: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 864
                    Iteration time: 1.00s
                      Time elapsed: 00:00:31
                               ETA: 00:01:25

################################################################################
                      [1m Learning iteration 27/100 [0m                       

                       Computation: 32 steps/s (collection: 0.855s, learning 0.142s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.2145
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 1.3531
                       Mean reward: -9.11
               Mean episode length: 205.25
              Episode_Reward/alive: 0.7633
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -3.0976
           Episode_Reward/cart_vel: -0.0145
           Episode_Reward/pole_vel: -0.0128
      Episode_Termination/time_out: 0.2500
Episode_Termination/cart_out_of_bounds: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 896
                    Iteration time: 1.00s
                      Time elapsed: 00:00:32
                               ETA: 00:01:23

################################################################################
                      [1m Learning iteration 28/100 [0m                       

                       Computation: 32 steps/s (collection: 0.839s, learning 0.146s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.6256
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 1.3519
                       Mean reward: -9.11
               Mean episode length: 205.25
              Episode_Reward/alive: 0.6667
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -2.8295
           Episode_Reward/cart_vel: -0.0129
           Episode_Reward/pole_vel: -0.0119
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 928
                    Iteration time: 0.99s
                      Time elapsed: 00:00:33
                               ETA: 00:01:22

################################################################################
                      [1m Learning iteration 29/100 [0m                       

                       Computation: 32 steps/s (collection: 0.837s, learning 0.136s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.3492
               Mean surrogate loss: -0.0385
                 Mean entropy loss: 1.3486
                       Mean reward: -9.11
               Mean episode length: 205.25
              Episode_Reward/alive: 0.6667
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -2.8295
           Episode_Reward/cart_vel: -0.0129
           Episode_Reward/pole_vel: -0.0119
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 960
                    Iteration time: 0.97s
                      Time elapsed: 00:00:34
                               ETA: 00:01:20

################################################################################
                      [1m Learning iteration 30/100 [0m                       

                       Computation: 32 steps/s (collection: 0.834s, learning 0.141s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0626
               Mean surrogate loss: -0.0278
                 Mean entropy loss: 1.3439
                       Mean reward: -9.11
               Mean episode length: 205.25
              Episode_Reward/alive: 0.6667
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -2.8295
           Episode_Reward/cart_vel: -0.0129
           Episode_Reward/pole_vel: -0.0119
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 992
                    Iteration time: 0.97s
                      Time elapsed: 00:00:35
                               ETA: 00:01:19

################################################################################
                      [1m Learning iteration 31/100 [0m                       

                       Computation: 32 steps/s (collection: 0.839s, learning 0.143s)
             Mean action noise std: 0.93
          Mean value_function loss: 1.1327
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 1.3441
                       Mean reward: -9.11
               Mean episode length: 205.25
              Episode_Reward/alive: 0.6667
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -2.8295
           Episode_Reward/cart_vel: -0.0129
           Episode_Reward/pole_vel: -0.0119
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1024
                    Iteration time: 0.98s
                      Time elapsed: 00:00:36
                               ETA: 00:01:17

################################################################################
                      [1m Learning iteration 32/100 [0m                       

                       Computation: 31 steps/s (collection: 0.854s, learning 0.147s)
             Mean action noise std: 0.93
          Mean value_function loss: 1.0699
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 1.3435
                       Mean reward: -8.13
               Mean episode length: 180.60
              Episode_Reward/alive: 0.5179
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -2.1791
           Episode_Reward/cart_vel: -0.0110
           Episode_Reward/pole_vel: -0.0090
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1056
                    Iteration time: 1.00s
                      Time elapsed: 00:00:37
                               ETA: 00:01:16

################################################################################
                      [1m Learning iteration 33/100 [0m                       

                       Computation: 32 steps/s (collection: 0.828s, learning 0.145s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.1180
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 1.3379
                       Mean reward: -8.13
               Mean episode length: 180.60
              Episode_Reward/alive: 0.2700
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -1.0950
           Episode_Reward/cart_vel: -0.0078
           Episode_Reward/pole_vel: -0.0042
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1088
                    Iteration time: 0.97s
                      Time elapsed: 00:00:38
                               ETA: 00:01:15

################################################################################
                      [1m Learning iteration 34/100 [0m                       

                       Computation: 32 steps/s (collection: 0.834s, learning 0.140s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.3090
               Mean surrogate loss: 0.0629
                 Mean entropy loss: 1.3249
                       Mean reward: -6.97
               Mean episode length: 156.00
              Episode_Reward/alive: 0.2190
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.8556
           Episode_Reward/cart_vel: -0.0075
           Episode_Reward/pole_vel: -0.0036
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1120
                    Iteration time: 0.97s
                      Time elapsed: 00:00:39
                               ETA: 00:01:13

################################################################################
                      [1m Learning iteration 35/100 [0m                       

                       Computation: 32 steps/s (collection: 0.846s, learning 0.148s)
             Mean action noise std: 0.92
          Mean value_function loss: 1.0199
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 1.3262
                       Mean reward: -8.06
               Mean episode length: 160.00
              Episode_Reward/alive: 0.5156
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -2.9031
           Episode_Reward/cart_vel: -0.0122
           Episode_Reward/pole_vel: -0.0082
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1152
                    Iteration time: 0.99s
                      Time elapsed: 00:00:40
                               ETA: 00:01:12

################################################################################
                      [1m Learning iteration 36/100 [0m                       

                       Computation: 32 steps/s (collection: 0.850s, learning 0.142s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.2293
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 1.3309
                       Mean reward: -6.38
               Mean episode length: 130.33
              Episode_Reward/alive: 0.3133
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -1.6259
           Episode_Reward/cart_vel: -0.0097
           Episode_Reward/pole_vel: -0.0053
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1184
                    Iteration time: 0.99s
                      Time elapsed: 00:00:41
                               ETA: 00:01:10

################################################################################
                      [1m Learning iteration 37/100 [0m                       

                       Computation: 32 steps/s (collection: 0.847s, learning 0.149s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.2401
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 1.3315
                       Mean reward: -5.77
               Mean episode length: 119.40
              Episode_Reward/alive: 0.0967
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.1434
           Episode_Reward/cart_vel: -0.0078
           Episode_Reward/pole_vel: -0.0025
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1216
                    Iteration time: 1.00s
                      Time elapsed: 00:00:42
                               ETA: 00:01:09

################################################################################
                      [1m Learning iteration 38/100 [0m                       

                       Computation: 32 steps/s (collection: 0.842s, learning 0.153s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0536
               Mean surrogate loss: -0.0341
                 Mean entropy loss: 1.3388
                       Mean reward: -4.86
               Mean episode length: 102.58
              Episode_Reward/alive: 0.0615
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0779
           Episode_Reward/cart_vel: -0.0067
           Episode_Reward/pole_vel: -0.0021
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1248
                    Iteration time: 1.00s
                      Time elapsed: 00:00:43
                               ETA: 00:01:08

################################################################################
                      [1m Learning iteration 39/100 [0m                       

                       Computation: 32 steps/s (collection: 0.845s, learning 0.140s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0732
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 1.3408
                       Mean reward: -4.22
               Mean episode length: 90.71
              Episode_Reward/alive: 0.0575
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.1297
           Episode_Reward/cart_vel: -0.0062
           Episode_Reward/pole_vel: -0.0018
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1280
                    Iteration time: 0.98s
                      Time elapsed: 00:00:44
                               ETA: 00:01:07

################################################################################
                      [1m Learning iteration 40/100 [0m                       

                       Computation: 32 steps/s (collection: 0.838s, learning 0.144s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0748
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 1.3377
                       Mean reward: -3.97
               Mean episode length: 86.00
              Episode_Reward/alive: 0.0508
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0743
           Episode_Reward/cart_vel: -0.0057
           Episode_Reward/pole_vel: -0.0016
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1312
                    Iteration time: 0.98s
                      Time elapsed: 00:00:44
                               ETA: 00:01:05

################################################################################
                      [1m Learning iteration 41/100 [0m                       

                       Computation: 32 steps/s (collection: 0.830s, learning 0.148s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.1645
               Mean surrogate loss: -0.0246
                 Mean entropy loss: 1.3296
                       Mean reward: -3.97
               Mean episode length: 86.00
              Episode_Reward/alive: 0.0633
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.1591
           Episode_Reward/cart_vel: -0.0076
           Episode_Reward/pole_vel: -0.0016
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1344
                    Iteration time: 0.98s
                      Time elapsed: 00:00:45
                               ETA: 00:01:04

################################################################################
                      [1m Learning iteration 42/100 [0m                       

                       Computation: 32 steps/s (collection: 0.843s, learning 0.141s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.2729
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 1.3287
                       Mean reward: -3.75
               Mean episode length: 81.94
              Episode_Reward/alive: 0.0665
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.1220
           Episode_Reward/cart_vel: -0.0088
           Episode_Reward/pole_vel: -0.0022
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376
                    Iteration time: 0.98s
                      Time elapsed: 00:00:46
                               ETA: 00:01:03

################################################################################
                      [1m Learning iteration 43/100 [0m                       

                       Computation: 32 steps/s (collection: 0.830s, learning 0.153s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.3517
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 1.3282
                       Mean reward: -3.52
               Mean episode length: 78.24
              Episode_Reward/alive: 0.0617
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0468
           Episode_Reward/cart_vel: -0.0078
           Episode_Reward/pole_vel: -0.0019
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1408
                    Iteration time: 0.98s
                      Time elapsed: 00:00:47
                               ETA: 00:01:02

################################################################################
                      [1m Learning iteration 44/100 [0m                       

                       Computation: 32 steps/s (collection: 0.825s, learning 0.154s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.4731
               Mean surrogate loss: -0.0163
                 Mean entropy loss: 1.3233
                       Mean reward: -3.32
               Mean episode length: 74.89
              Episode_Reward/alive: 0.0579
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0235
           Episode_Reward/cart_vel: -0.0070
           Episode_Reward/pole_vel: -0.0016
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1440
                    Iteration time: 0.98s
                      Time elapsed: 00:00:48
                               ETA: 00:01:00

################################################################################
                      [1m Learning iteration 45/100 [0m                       

                       Computation: 32 steps/s (collection: 0.843s, learning 0.144s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.4203
               Mean surrogate loss: -0.0208
                 Mean entropy loss: 1.3177
                       Mean reward: -3.16
               Mean episode length: 71.89
              Episode_Reward/alive: 0.0567
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0553
           Episode_Reward/cart_vel: -0.0073
           Episode_Reward/pole_vel: -0.0017
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1472
                    Iteration time: 0.99s
                      Time elapsed: 00:00:49
                               ETA: 00:00:59

################################################################################
                      [1m Learning iteration 46/100 [0m                       

                       Computation: 32 steps/s (collection: 0.859s, learning 0.139s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.4206
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 1.3148
                       Mean reward: -3.03
               Mean episode length: 70.57
              Episode_Reward/alive: 0.1183
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.3141
           Episode_Reward/cart_vel: -0.0066
           Episode_Reward/pole_vel: -0.0033
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1504
                    Iteration time: 1.00s
                      Time elapsed: 00:00:50
                               ETA: 00:00:58

################################################################################
                      [1m Learning iteration 47/100 [0m                       

                       Computation: 32 steps/s (collection: 0.842s, learning 0.148s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0783
               Mean surrogate loss: -0.0281
                 Mean entropy loss: 1.3119
                       Mean reward: -2.78
               Mean episode length: 65.96
              Episode_Reward/alive: 0.0515
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0874
           Episode_Reward/cart_vel: -0.0077
           Episode_Reward/pole_vel: -0.0017
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1536
                    Iteration time: 0.99s
                      Time elapsed: 00:00:51
                               ETA: 00:00:57

################################################################################
                      [1m Learning iteration 48/100 [0m                       

                       Computation: 32 steps/s (collection: 0.840s, learning 0.145s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0046
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 1.3116
                       Mean reward: -2.58
               Mean episode length: 62.24
              Episode_Reward/alive: 0.0498
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.1021
           Episode_Reward/cart_vel: -0.0071
           Episode_Reward/pole_vel: -0.0016
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1568
                    Iteration time: 0.99s
                      Time elapsed: 00:00:52
                               ETA: 00:00:56

################################################################################
                      [1m Learning iteration 49/100 [0m                       

                       Computation: 32 steps/s (collection: 0.840s, learning 0.148s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0594
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 1.3297
                       Mean reward: -2.50
               Mean episode length: 60.69
              Episode_Reward/alive: 0.0738
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.1295
           Episode_Reward/cart_vel: -0.0063
           Episode_Reward/pole_vel: -0.0019
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1600
                    Iteration time: 0.99s
                      Time elapsed: 00:00:53
                               ETA: 00:00:54

################################################################################
                      [1m Learning iteration 50/100 [0m                       

                       Computation: 32 steps/s (collection: 0.836s, learning 0.148s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0550
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 1.3326
                       Mean reward: -2.41
               Mean episode length: 59.22
              Episode_Reward/alive: 0.0675
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0861
           Episode_Reward/cart_vel: -0.0057
           Episode_Reward/pole_vel: -0.0018
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1632
                    Iteration time: 0.98s
                      Time elapsed: 00:00:54
                               ETA: 00:00:53

################################################################################
                      [1m Learning iteration 51/100 [0m                       

                       Computation: 32 steps/s (collection: 0.841s, learning 0.142s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 1.3301
                       Mean reward: -2.26
               Mean episode length: 56.79
              Episode_Reward/alive: 0.0694
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.1041
           Episode_Reward/cart_vel: -0.0054
           Episode_Reward/pole_vel: -0.0017
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1664
                    Iteration time: 0.98s
                      Time elapsed: 00:00:55
                               ETA: 00:00:52

################################################################################
                      [1m Learning iteration 52/100 [0m                       

                       Computation: 32 steps/s (collection: 0.846s, learning 0.147s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0167
                 Mean entropy loss: 1.3198
                       Mean reward: -2.20
               Mean episode length: 55.40
              Episode_Reward/alive: 0.0508
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.1353
           Episode_Reward/cart_vel: -0.0066
           Episode_Reward/pole_vel: -0.0015
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1696
                    Iteration time: 0.99s
                      Time elapsed: 00:00:56
                               ETA: 00:00:51

################################################################################
                      [1m Learning iteration 53/100 [0m                       

                       Computation: 32 steps/s (collection: 0.835s, learning 0.142s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0413
                 Mean entropy loss: 1.3161
                       Mean reward: -2.08
               Mean episode length: 53.69
              Episode_Reward/alive: 0.0763
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.1718
           Episode_Reward/cart_vel: -0.0067
           Episode_Reward/pole_vel: -0.0020
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1728
                    Iteration time: 0.98s
                      Time elapsed: 00:00:57
                               ETA: 00:00:50

################################################################################
                      [1m Learning iteration 54/100 [0m                       

                       Computation: 32 steps/s (collection: 0.846s, learning 0.146s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0202
               Mean surrogate loss: -0.0483
                 Mean entropy loss: 1.3085
                       Mean reward: -2.02
               Mean episode length: 52.52
              Episode_Reward/alive: 0.0765
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0469
           Episode_Reward/cart_vel: -0.0057
           Episode_Reward/pole_vel: -0.0021
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1760
                    Iteration time: 0.99s
                      Time elapsed: 00:00:58
                               ETA: 00:00:49

################################################################################
                      [1m Learning iteration 55/100 [0m                       

                       Computation: 32 steps/s (collection: 0.822s, learning 0.148s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.2877
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 1.3071
                       Mean reward: -2.02
               Mean episode length: 52.52
              Episode_Reward/alive: 0.0467
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0384
           Episode_Reward/cart_vel: -0.0053
           Episode_Reward/pole_vel: -0.0017
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1792
                    Iteration time: 0.97s
                      Time elapsed: 00:00:59
                               ETA: 00:00:48

################################################################################
                      [1m Learning iteration 56/100 [0m                       

                       Computation: 32 steps/s (collection: 0.837s, learning 0.146s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.2876
               Mean surrogate loss: 0.0159
                 Mean entropy loss: 1.3152
                       Mean reward: -1.96
               Mean episode length: 51.65
              Episode_Reward/alive: 0.0667
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0782
           Episode_Reward/cart_vel: -0.0071
           Episode_Reward/pole_vel: -0.0022
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1824
                    Iteration time: 0.98s
                      Time elapsed: 00:01:00
                               ETA: 00:00:46

################################################################################
                      [1m Learning iteration 57/100 [0m                       

                       Computation: 32 steps/s (collection: 0.845s, learning 0.138s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 1.3192
                       Mean reward: -1.92
               Mean episode length: 50.80
              Episode_Reward/alive: 0.0721
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.1195
           Episode_Reward/cart_vel: -0.0069
           Episode_Reward/pole_vel: -0.0022
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1856
                    Iteration time: 0.98s
                      Time elapsed: 00:01:01
                               ETA: 00:00:45

################################################################################
                      [1m Learning iteration 58/100 [0m                       

                       Computation: 31 steps/s (collection: 0.850s, learning 0.155s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0197
                 Mean entropy loss: 1.3217
                       Mean reward: -1.83
               Mean episode length: 49.53
              Episode_Reward/alive: 0.1479
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.3724
           Episode_Reward/cart_vel: -0.0111
           Episode_Reward/pole_vel: -0.0034
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1888
                    Iteration time: 1.00s
                      Time elapsed: 00:01:02
                               ETA: 00:00:44

################################################################################
                      [1m Learning iteration 59/100 [0m                       

                       Computation: 32 steps/s (collection: 0.844s, learning 0.148s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0476
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 1.3207
                       Mean reward: -1.79
               Mean episode length: 48.69
              Episode_Reward/alive: 0.0471
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0225
           Episode_Reward/cart_vel: -0.0051
           Episode_Reward/pole_vel: -0.0015
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1920
                    Iteration time: 0.99s
                      Time elapsed: 00:01:03
                               ETA: 00:00:43

################################################################################
                      [1m Learning iteration 60/100 [0m                       

                       Computation: 32 steps/s (collection: 0.836s, learning 0.140s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0341
                 Mean entropy loss: 1.3204
                       Mean reward: -1.75
               Mean episode length: 47.98
              Episode_Reward/alive: 0.0615
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0665
           Episode_Reward/cart_vel: -0.0062
           Episode_Reward/pole_vel: -0.0021
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1952
                    Iteration time: 0.98s
                      Time elapsed: 00:01:04
                               ETA: 00:00:42

################################################################################
                      [1m Learning iteration 61/100 [0m                       

                       Computation: 32 steps/s (collection: 0.847s, learning 0.140s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0349
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 1.3235
                       Mean reward: -1.67
               Mean episode length: 46.67
              Episode_Reward/alive: 0.0575
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0906
           Episode_Reward/cart_vel: -0.0055
           Episode_Reward/pole_vel: -0.0019
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1984
                    Iteration time: 0.99s
                      Time elapsed: 00:01:05
                               ETA: 00:00:41

################################################################################
                      [1m Learning iteration 62/100 [0m                       

                       Computation: 1 steps/s (collection: 19.882s, learning 0.139s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0138
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 1.3456
                       Mean reward: -1.59
               Mean episode length: 45.36
              Episode_Reward/alive: 0.0571
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0363
           Episode_Reward/cart_vel: -0.0057
           Episode_Reward/pole_vel: -0.0017
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2016
                    Iteration time: 20.02s
                      Time elapsed: 00:01:25
                               ETA: 00:00:51

################################################################################
                      [1m Learning iteration 63/100 [0m                       

                       Computation: 98 steps/s (collection: 0.180s, learning 0.144s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0534
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 1.3488
                       Mean reward: -1.53
               Mean episode length: 44.17
              Episode_Reward/alive: 0.0560
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0908
           Episode_Reward/cart_vel: -0.0056
           Episode_Reward/pole_vel: -0.0018
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2048
                    Iteration time: 0.32s
                      Time elapsed: 00:01:26
                               ETA: 00:00:49

################################################################################
                      [1m Learning iteration 64/100 [0m                       

                       Computation: 103 steps/s (collection: 0.174s, learning 0.134s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0125
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 1.3129
                       Mean reward: -1.47
               Mean episode length: 43.02
              Episode_Reward/alive: 0.0506
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0428
           Episode_Reward/cart_vel: -0.0063
           Episode_Reward/pole_vel: -0.0018
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2080
                    Iteration time: 0.31s
                      Time elapsed: 00:01:26
                               ETA: 00:00:47

################################################################################
                      [1m Learning iteration 65/100 [0m                       

                       Computation: 104 steps/s (collection: 0.159s, learning 0.146s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0231
                 Mean entropy loss: 1.3050
                       Mean reward: -1.44
               Mean episode length: 42.57
              Episode_Reward/alive: 0.0623
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0317
           Episode_Reward/cart_vel: -0.0060
           Episode_Reward/pole_vel: -0.0016
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2112
                    Iteration time: 0.31s
                      Time elapsed: 00:01:26
                               ETA: 00:00:45

################################################################################
                      [1m Learning iteration 66/100 [0m                       

                       Computation: 110 steps/s (collection: 0.147s, learning 0.142s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 1.3045
                       Mean reward: -1.39
               Mean episode length: 41.98
              Episode_Reward/alive: 0.0698
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0412
           Episode_Reward/cart_vel: -0.0055
           Episode_Reward/pole_vel: -0.0015
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2144
                    Iteration time: 0.29s
                      Time elapsed: 00:01:26
                               ETA: 00:00:44

################################################################################
                      [1m Learning iteration 67/100 [0m                       

                       Computation: 111 steps/s (collection: 0.150s, learning 0.136s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0069
               Mean surrogate loss: -0.0232
                 Mean entropy loss: 1.3062
                       Mean reward: -1.37
               Mean episode length: 41.46
              Episode_Reward/alive: 0.0683
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0275
           Episode_Reward/cart_vel: -0.0065
           Episode_Reward/pole_vel: -0.0018
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2176
                    Iteration time: 0.29s
                      Time elapsed: 00:01:27
                               ETA: 00:00:42

################################################################################
                      [1m Learning iteration 68/100 [0m                       

                       Computation: 106 steps/s (collection: 0.159s, learning 0.141s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0095
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 1.3077
                       Mean reward: -1.34
               Mean episode length: 41.08
              Episode_Reward/alive: 0.0642
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0591
           Episode_Reward/cart_vel: -0.0076
           Episode_Reward/pole_vel: -0.0019
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2208
                    Iteration time: 0.30s
                      Time elapsed: 00:01:27
                               ETA: 00:00:40

################################################################################
                      [1m Learning iteration 69/100 [0m                       

                       Computation: 109 steps/s (collection: 0.153s, learning 0.138s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0069
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 1.3077
                       Mean reward: -1.30
               Mean episode length: 40.24
              Episode_Reward/alive: 0.0546
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0726
           Episode_Reward/cart_vel: -0.0057
           Episode_Reward/pole_vel: -0.0018
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2240
                    Iteration time: 0.29s
                      Time elapsed: 00:01:27
                               ETA: 00:00:38

################################################################################
                      [1m Learning iteration 70/100 [0m                       

                       Computation: 108 steps/s (collection: 0.162s, learning 0.134s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0100
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 1.3070
                       Mean reward: -1.26
               Mean episode length: 39.58
              Episode_Reward/alive: 0.0646
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0794
           Episode_Reward/cart_vel: -0.0072
           Episode_Reward/pole_vel: -0.0017
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2272
                    Iteration time: 0.30s
                      Time elapsed: 00:01:28
                               ETA: 00:00:37

################################################################################
                      [1m Learning iteration 71/100 [0m                       

                       Computation: 110 steps/s (collection: 0.157s, learning 0.134s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0038
               Mean surrogate loss: 0.0441
                 Mean entropy loss: 1.3026
                       Mean reward: -1.23
               Mean episode length: 38.75
              Episode_Reward/alive: 0.0550
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0758
           Episode_Reward/cart_vel: -0.0082
           Episode_Reward/pole_vel: -0.0016
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2304
                    Iteration time: 0.29s
                      Time elapsed: 00:01:28
                               ETA: 00:00:35

################################################################################
                      [1m Learning iteration 72/100 [0m                       

                       Computation: 106 steps/s (collection: 0.164s, learning 0.136s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0091
               Mean surrogate loss: -0.0205
                 Mean entropy loss: 1.3020
                       Mean reward: -1.20
               Mean episode length: 37.89
              Episode_Reward/alive: 0.0377
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0581
           Episode_Reward/cart_vel: -0.0053
           Episode_Reward/pole_vel: -0.0013
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2336
                    Iteration time: 0.30s
                      Time elapsed: 00:01:28
                               ETA: 00:00:34

################################################################################
                      [1m Learning iteration 73/100 [0m                       

                       Computation: 100 steps/s (collection: 0.172s, learning 0.145s)
             Mean action noise std: 0.88
          Mean value_function loss: 0.0069
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 1.3003
                       Mean reward: -1.14
               Mean episode length: 36.77
              Episode_Reward/alive: 0.0483
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0332
           Episode_Reward/cart_vel: -0.0073
           Episode_Reward/pole_vel: -0.0015
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2368
                    Iteration time: 0.32s
                      Time elapsed: 00:01:29
                               ETA: 00:00:32

################################################################################
                      [1m Learning iteration 74/100 [0m                       

                       Computation: 107 steps/s (collection: 0.156s, learning 0.141s)
             Mean action noise std: 0.88
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0159
                 Mean entropy loss: 1.2884
                       Mean reward: -1.10
               Mean episode length: 35.73
              Episode_Reward/alive: 0.0427
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0871
           Episode_Reward/cart_vel: -0.0074
           Episode_Reward/pole_vel: -0.0014
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2400
                    Iteration time: 0.30s
                      Time elapsed: 00:01:29
                               ETA: 00:00:30

################################################################################
                      [1m Learning iteration 75/100 [0m                       

                       Computation: 108 steps/s (collection: 0.153s, learning 0.143s)
             Mean action noise std: 0.88
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0190
                 Mean entropy loss: 1.2900
                       Mean reward: -1.07
               Mean episode length: 35.07
              Episode_Reward/alive: 0.0400
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0470
           Episode_Reward/cart_vel: -0.0062
           Episode_Reward/pole_vel: -0.0015
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2432
                    Iteration time: 0.30s
                      Time elapsed: 00:01:29
                               ETA: 00:00:29

################################################################################
                      [1m Learning iteration 76/100 [0m                       

                       Computation: 105 steps/s (collection: 0.165s, learning 0.138s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0127
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 1.3024
                       Mean reward: -1.05
               Mean episode length: 34.45
              Episode_Reward/alive: 0.0396
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0412
           Episode_Reward/cart_vel: -0.0064
           Episode_Reward/pole_vel: -0.0015
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2464
                    Iteration time: 0.30s
                      Time elapsed: 00:01:29
                               ETA: 00:00:28

################################################################################
                      [1m Learning iteration 77/100 [0m                       

                       Computation: 103 steps/s (collection: 0.167s, learning 0.141s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0115
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 1.3031
                       Mean reward: -1.01
               Mean episode length: 33.66
              Episode_Reward/alive: 0.0456
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.1086
           Episode_Reward/cart_vel: -0.0074
           Episode_Reward/pole_vel: -0.0016
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2496
                    Iteration time: 0.31s
                      Time elapsed: 00:01:30
                               ETA: 00:00:26

################################################################################
                      [1m Learning iteration 78/100 [0m                       

                       Computation: 108 steps/s (collection: 0.154s, learning 0.140s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0070
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 1.3023
                       Mean reward: -0.99
               Mean episode length: 33.14
              Episode_Reward/alive: 0.0438
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0308
           Episode_Reward/cart_vel: -0.0062
           Episode_Reward/pole_vel: -0.0015
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2528
                    Iteration time: 0.29s
                      Time elapsed: 00:01:30
                               ETA: 00:00:25

################################################################################
                      [1m Learning iteration 79/100 [0m                       

                       Computation: 107 steps/s (collection: 0.160s, learning 0.137s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0197
                 Mean entropy loss: 1.3036
                       Mean reward: -0.96
               Mean episode length: 32.67
              Episode_Reward/alive: 0.0421
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0172
           Episode_Reward/cart_vel: -0.0068
           Episode_Reward/pole_vel: -0.0014
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2560
                    Iteration time: 0.30s
                      Time elapsed: 00:01:30
                               ETA: 00:00:23

################################################################################
                      [1m Learning iteration 80/100 [0m                       

                       Computation: 104 steps/s (collection: 0.164s, learning 0.142s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0241
                 Mean entropy loss: 1.3034
                       Mean reward: -0.93
               Mean episode length: 31.91
              Episode_Reward/alive: 0.0410
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0461
           Episode_Reward/cart_vel: -0.0067
           Episode_Reward/pole_vel: -0.0015
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2592
                    Iteration time: 0.31s
                      Time elapsed: 00:01:31
                               ETA: 00:00:22

################################################################################
                      [1m Learning iteration 81/100 [0m                       

                       Computation: 108 steps/s (collection: 0.158s, learning 0.138s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 1.3015
                       Mean reward: -0.91
               Mean episode length: 31.47
              Episode_Reward/alive: 0.0371
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0226
           Episode_Reward/cart_vel: -0.0057
           Episode_Reward/pole_vel: -0.0013
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2624
                    Iteration time: 0.30s
                      Time elapsed: 00:01:31
                               ETA: 00:00:21

################################################################################
                      [1m Learning iteration 82/100 [0m                       

                       Computation: 105 steps/s (collection: 0.160s, learning 0.142s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0130
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 1.2992
                       Mean reward: -0.89
               Mean episode length: 31.07
              Episode_Reward/alive: 0.0410
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0677
           Episode_Reward/cart_vel: -0.0070
           Episode_Reward/pole_vel: -0.0015
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2656
                    Iteration time: 0.30s
                      Time elapsed: 00:01:31
                               ETA: 00:00:19

################################################################################
                      [1m Learning iteration 83/100 [0m                       

                       Computation: 106 steps/s (collection: 0.163s, learning 0.138s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0178
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 1.3003
                       Mean reward: -0.87
               Mean episode length: 30.48
              Episode_Reward/alive: 0.0460
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0906
           Episode_Reward/cart_vel: -0.0078
           Episode_Reward/pole_vel: -0.0016
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2688
                    Iteration time: 0.30s
                      Time elapsed: 00:01:32
                               ETA: 00:00:18

################################################################################
                      [1m Learning iteration 84/100 [0m                       

                       Computation: 107 steps/s (collection: 0.153s, learning 0.144s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0075
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 1.3002
                       Mean reward: -0.86
               Mean episode length: 30.13
              Episode_Reward/alive: 0.0394
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0549
           Episode_Reward/cart_vel: -0.0065
           Episode_Reward/pole_vel: -0.0015
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2720
                    Iteration time: 0.30s
                      Time elapsed: 00:01:32
                               ETA: 00:00:17

################################################################################
                      [1m Learning iteration 85/100 [0m                       

                       Computation: 104 steps/s (collection: 0.165s, learning 0.140s)
             Mean action noise std: 0.88
          Mean value_function loss: 0.0088
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 1.2971
                       Mean reward: -0.84
               Mean episode length: 29.74
              Episode_Reward/alive: 0.0383
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0276
           Episode_Reward/cart_vel: -0.0061
           Episode_Reward/pole_vel: -0.0013
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752
                    Iteration time: 0.31s
                      Time elapsed: 00:01:32
                               ETA: 00:00:16

################################################################################
                      [1m Learning iteration 86/100 [0m                       

                       Computation: 104 steps/s (collection: 0.166s, learning 0.141s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 1.2960
                       Mean reward: -0.82
               Mean episode length: 29.40
              Episode_Reward/alive: 0.0429
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0269
           Episode_Reward/cart_vel: -0.0076
           Episode_Reward/pole_vel: -0.0015
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2784
                    Iteration time: 0.31s
                      Time elapsed: 00:01:32
                               ETA: 00:00:14

################################################################################
                      [1m Learning iteration 87/100 [0m                       

                       Computation: 103 steps/s (collection: 0.166s, learning 0.143s)
             Mean action noise std: 0.82
          Mean value_function loss: 0.0114
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 1.2792
                       Mean reward: -0.81
               Mean episode length: 29.14
              Episode_Reward/alive: 0.0510
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0483
           Episode_Reward/cart_vel: -0.0073
           Episode_Reward/pole_vel: -0.0016
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2816
                    Iteration time: 0.31s
                      Time elapsed: 00:01:33
                               ETA: 00:00:13

################################################################################
                      [1m Learning iteration 88/100 [0m                       

                       Computation: 105 steps/s (collection: 0.162s, learning 0.142s)
             Mean action noise std: 0.81
          Mean value_function loss: 0.0043
               Mean surrogate loss: 0.0234
                 Mean entropy loss: 1.2137
                       Mean reward: -0.80
               Mean episode length: 28.96
              Episode_Reward/alive: 0.0408
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0594
           Episode_Reward/cart_vel: -0.0060
           Episode_Reward/pole_vel: -0.0015
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2848
                    Iteration time: 0.30s
                      Time elapsed: 00:01:33
                               ETA: 00:00:12

################################################################################
                      [1m Learning iteration 89/100 [0m                       

                       Computation: 106 steps/s (collection: 0.161s, learning 0.141s)
             Mean action noise std: 0.81
          Mean value_function loss: 0.0115
               Mean surrogate loss: -0.0086
                 Mean entropy loss: 1.2133
                       Mean reward: -0.79
               Mean episode length: 28.97
              Episode_Reward/alive: 0.0679
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.1296
           Episode_Reward/cart_vel: -0.0066
           Episode_Reward/pole_vel: -0.0016
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2880
                    Iteration time: 0.30s
                      Time elapsed: 00:01:33
                               ETA: 00:00:11

################################################################################
                      [1m Learning iteration 90/100 [0m                       

                       Computation: 109 steps/s (collection: 0.150s, learning 0.141s)
             Mean action noise std: 0.82
          Mean value_function loss: 0.0093
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 1.2162
                       Mean reward: -0.75
               Mean episode length: 27.79
              Episode_Reward/alive: 0.0877
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.1607
           Episode_Reward/cart_vel: -0.0076
           Episode_Reward/pole_vel: -0.0019
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2912
                    Iteration time: 0.29s
                      Time elapsed: 00:01:34
                               ETA: 00:00:10

################################################################################
                      [1m Learning iteration 91/100 [0m                       

                       Computation: 103 steps/s (collection: 0.167s, learning 0.143s)
             Mean action noise std: 0.86
          Mean value_function loss: 0.0052
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 1.2455
                       Mean reward: -0.55
               Mean episode length: 23.11
              Episode_Reward/alive: 0.0473
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.1012
           Episode_Reward/cart_vel: -0.0074
           Episode_Reward/pole_vel: -0.0017
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2944
                    Iteration time: 0.31s
                      Time elapsed: 00:01:34
                               ETA: 00:00:09

################################################################################
                      [1m Learning iteration 92/100 [0m                       

                       Computation: 108 steps/s (collection: 0.155s, learning 0.141s)
             Mean action noise std: 0.88
          Mean value_function loss: 0.0065
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 1.2812
                       Mean reward: -0.40
               Mean episode length: 20.56
              Episode_Reward/alive: 0.0444
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0890
           Episode_Reward/cart_vel: -0.0075
           Episode_Reward/pole_vel: -0.0016
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2976
                    Iteration time: 0.30s
                      Time elapsed: 00:01:34
                               ETA: 00:00:08

################################################################################
                      [1m Learning iteration 93/100 [0m                       

                       Computation: 110 steps/s (collection: 0.151s, learning 0.137s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0058
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 1.2938
                       Mean reward: -0.24
               Mean episode length: 18.70
              Episode_Reward/alive: 0.0460
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0694
           Episode_Reward/cart_vel: -0.0071
           Episode_Reward/pole_vel: -0.0017
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3008
                    Iteration time: 0.29s
                      Time elapsed: 00:01:35
                               ETA: 00:00:07

################################################################################
                      [1m Learning iteration 94/100 [0m                       

                       Computation: 105 steps/s (collection: 0.165s, learning 0.139s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0025
               Mean surrogate loss: 0.0126
                 Mean entropy loss: 1.3140
                       Mean reward: -0.23
               Mean episode length: 18.47
              Episode_Reward/alive: 0.0460
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0460
           Episode_Reward/cart_vel: -0.0066
           Episode_Reward/pole_vel: -0.0017
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3040
                    Iteration time: 0.30s
                      Time elapsed: 00:01:35
                               ETA: 00:00:06

################################################################################
                      [1m Learning iteration 95/100 [0m                       

                       Computation: 102 steps/s (collection: 0.175s, learning 0.138s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0063
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 1.3263
                       Mean reward: -0.23
               Mean episode length: 18.39
              Episode_Reward/alive: 0.0467
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0535
           Episode_Reward/cart_vel: -0.0058
           Episode_Reward/pole_vel: -0.0017
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3072
                    Iteration time: 0.31s
                      Time elapsed: 00:01:35
                               ETA: 00:00:04

################################################################################
                      [1m Learning iteration 96/100 [0m                       

                       Computation: 103 steps/s (collection: 0.162s, learning 0.145s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0122
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 1.3264
                       Mean reward: -0.23
               Mean episode length: 18.29
              Episode_Reward/alive: 0.0494
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0719
           Episode_Reward/cart_vel: -0.0064
           Episode_Reward/pole_vel: -0.0017
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3104
                    Iteration time: 0.31s
                      Time elapsed: 00:01:35
                               ETA: 00:00:03

################################################################################
                      [1m Learning iteration 97/100 [0m                       

                       Computation: 102 steps/s (collection: 0.169s, learning 0.143s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0054
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 1.3311
                       Mean reward: -0.22
               Mean episode length: 18.24
              Episode_Reward/alive: 0.0479
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0735
           Episode_Reward/cart_vel: -0.0056
           Episode_Reward/pole_vel: -0.0016
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3136
                    Iteration time: 0.31s
                      Time elapsed: 00:01:36
                               ETA: 00:00:02

################################################################################
                      [1m Learning iteration 98/100 [0m                       

                       Computation: 101 steps/s (collection: 0.171s, learning 0.145s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0048
               Mean surrogate loss: -0.0257
                 Mean entropy loss: 1.3376
                       Mean reward: -0.22
               Mean episode length: 18.20
              Episode_Reward/alive: 0.0517
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0803
           Episode_Reward/cart_vel: -0.0073
           Episode_Reward/pole_vel: -0.0018
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3168
                    Iteration time: 0.32s
                      Time elapsed: 00:01:36
                               ETA: 00:00:01

################################################################################
                      [1m Learning iteration 99/100 [0m                       

                       Computation: 108 steps/s (collection: 0.157s, learning 0.139s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0097
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 1.3398
                       Mean reward: -0.23
               Mean episode length: 18.18
              Episode_Reward/alive: 0.0479
        Episode_Reward/terminating: -0.0067
           Episode_Reward/pole_pos: -0.0619
           Episode_Reward/cart_vel: -0.0058
           Episode_Reward/pole_vel: -0.0016
      Episode_Termination/time_out: 0.0000
Episode_Termination/cart_out_of_bounds: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3200
                    Iteration time: 0.30s
                      Time elapsed: 00:01:36
                               ETA: 00:00:00

Training time: 99.88 seconds
2026-02-08T05:11:24Z [44,166ms] [Warning] [carb.scenerenderer-rtx.plugin] Failed to create NGX context.
2026-02-08T05:11:27Z [47,251ms] [Warning] [omni.fabric.plugin] Warning: attribute viewportHandle not found for bucket id 7

2026-02-08T05:11:37Z [57,295ms] [Warning] [omni.fabric.plugin] Warning: input prim:attr `/World/envs/env_0:omni:rtx:skip` has no valid data.

2026-02-08T05:11:50Z [69,492ms] [Warning] [omni.syntheticdata.plugin] OgnSdPostRenderVarToHost : rendervar copy from texture directly to host buffer is counter-performant. Please use copy from texture to device buffer first.
2026-02-08T05:11:57Z [76,416ms] [Warning] [carb] Client gpu.foundation.plugin has acquired [gpu::unstable::IMemoryBudgetManagerFactory v0.1] 100 times. Consider accessing this interface with carb::getCachedInterface() (Performance warning)
(run_singularity.py): Return
